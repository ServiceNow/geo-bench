experiment:
  generate_experiment_dir: /mnt/data/experiments/nils/new_classification_beyond_rgb
  # generate_experiment_dir: /mnt/data/experiments/pau/partition_sweeps_classification
  experiment_name: 1.00x_train # prefix
  experiment_type: sweep # standard, seeded_runs, sweep
  benchmark_dir: /mnt/data/cc_benchmark/classification_v0.7
  partition_name: 1.00x_train
wandb:
  project: ccb # wandb project name
  entity: climate-benchmark # user or team entity that hosts a project
  sweep:
    sweep_config_path: /mnt/home/climate-change-benchmark/ccb/torch_toolbox/wandb/hparams_classification_resnet18.yaml # or maybe define the entire sweep here?
    num_agents: 2 # how many agents participate in sweep, one agent usually corresponds to one gpu
    num_trials_per_agent: 2 # how many hparam trials each agent should execute, num_agents x num_trials_per_agent corresponds to total number of trials executed by sweep
model:
  model_generator_module_name: ccb.torch_toolbox.model_generators.timm_generator
  head_type: linear # classification
  loss_type: crossentropy
  backbone: resnet18  # resnet18, convnext_base, vit_tiny_patch16_224, vit_small_patch16_224. swinv2_tiny_window16_256
  default_input_size: [224, 224]
  pretrained: True
  new_channel_init_method: random
  lr_backbone: 1.0e-6
  lr_head: 1.0e-4
  optimizer: sgd
  head_type: linear # classification
  loss_type: crossentropy
  batch_size: 2
dataset:
  band_names: all
  format: hdf5
dataloader:
  num_workers: 4
pl: # all flags for pytorch lightning Trainer module, see https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
  gpus: 1
  accelerator: gpu
  max_epochs: 600
  max_steps: -1
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  val_check_interval: 0.25
  deterministic: False
  log_every_n_steps: 10
  enable_progress_bar: False